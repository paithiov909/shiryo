---
title: "文体的特徴にもとづく青空文庫の作品の著者分類"
author: "paithiov909"
date: "`r Sys.Date()`"
output:
  cleanrmd::html_document_clean:
    theme: "water"
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  tidy = "styler",
  collapse = TRUE,
  comment = "#>"
)

metathis::meta() |>
  metathis::meta_social(
    title = "文体的特徴にもとづく青空文庫の作品の著者分類",
    url = "https://paithiov909.github.io/shiryo/aozora/",
    og_type = "article",
    og_locale = "ja_JP",
    twitter_card_type =  "summary",
    twitter_creator = "@paithiov909"
  )

set.seed(53)
```

## はじめに

この記事では、青空文庫にある作品について、「助詞-記号」「助動詞-記号」「接続詞-記号」のパターンのマイニングをします。その後に、いくつかのパターンの出現比率を特徴量として、芥川龍之介（芥川竜之介）と太宰治の2例について、ロジスティック回帰で著者分類を試します。

## データの準備

### 前処理

ここでは、青空文庫にある以下の10人の作家の文章のうち「新字新仮名」で書かれているもののなかから一部を収集したデータセットを使います。

- 芥川龍之介（芥川竜之介）
- 太宰治
- 泉鏡花
- 菊池寛
- 森鴎外
- 夏目漱石
- 岡本綺堂
- 佐々木味津三
- 島崎藤村
- 海野十三

作品ごとの分量にかなりばらつきがあるので、このなかから文字数が5,000字未満のものを100篇だけランダムに抽出して分析します。

```{r download_file}
tmp <- tempfile(fileext = ".zip")
download.file("https://github.com/paithiov909/shiryo/raw/main/data/aozora.csv.zip", tmp)

df <- readr::read_csv(tmp, col_types = "cccf") |>
  dplyr::mutate(author = as.factor(author)) |>
  dplyr::filter(nchar(text) < 5000) |>
  dplyr::slice_sample(n = 100L)
```

作家ごとの大まかな文章量は次のようになっています。作家によって作品数にやや偏りがあります。

```{r summary_all}
df |>
  dplyr::mutate(nchar = nchar(text)) |>
  dplyr::group_by(author) |> 
  dplyr::summarise(nchar_mean = mean(nchar),
                   nchar_median = median(nchar),
                   nchar_min = min(nchar),
                   nchar_max = max(nchar),
                   n = dplyr::n()) |> 
  dplyr::mutate(across(where(is.numeric), trunc))
```

`audubon::strj_normalize`で文字列を正規化したうえで、[gibasa](https://github.com/paithiov909/gibasa)で分かち書きにします。

```{r tokenize_all}
df <- df |>
  dplyr::mutate(doc_id = title, text = audubon::strj_normalize(text)) |>
  dplyr::select(doc_id, text, author) |>
  gibasa::tokenize(split = TRUE) |>
  gibasa::prettify(col_select = "POS1")

summary(df)
```

### 出現頻度の集計

30回以上出現するパターンをグラフにします。「た-。」という文末の表現や「は-、」という主題を表したりする表現が多いようです。

```{r plot_stats}
require(ggplot2)

df |>
  dplyr::group_by(doc_id) |>
  dplyr::mutate(POS1 = paste(POS1, dplyr::lead(POS1), sep = "-"),
                token = paste(token, dplyr::lead(token), sep = "-")) |>
  dplyr::ungroup() |>
  dplyr::filter(POS1 %in% c("助詞-記号", "助動詞-記号", "接続詞-記号")) |>
  dplyr::count(token) |>
  dplyr::filter(n >= 30) |>
  ggpubr::ggdotchart(x = "token", y = "n",
                     rotate = TRUE, sorting = "descending",
                     ggtheme = theme_bw())
```

### 共起ネットワーク

共起関係についてネットワーク図を描いてみます。一部の語は「、」と「。」のどちらとも共起するようです。

```{r plot_network}
df |>
  dplyr::group_by(doc_id) |>
  dplyr::mutate(POS1 = paste(POS1, dplyr::lead(POS1), sep = "-"),
                from = token, to = dplyr::lead(token)) |>
  dplyr::ungroup() |>
  dplyr::filter(POS1 %in% c("助詞-記号", "助動詞-記号", "接続詞-記号")) |>
  dplyr::count(from, to) |>
  dplyr::filter(n > 5) |>
  tidygraph::as_tbl_graph() |>
  ggraph::ggraph(layout = "kk") + 
  ggraph::geom_edge_link(aes(width = n), alpha = .8) + 
  ggraph::scale_edge_width(range = c(.1, 1)) +
  ggraph::geom_node_point() +
  ggraph::geom_node_text(aes(label = name), repel = TRUE) +
  theme_bw()
```

### バルーンプロット

作家ごとに頻度を比較できるようにプロットしてみます（もっとも、青空文庫には極端に短い作品がたまにあったりするので、単純な頻度を比較するのはあまり適切でないかもしれません）。

なお、たとえば「神は言っている、ここで死ぬ定めではないと。」のような倒置で文末にくる「と-。」という表現や、「も-。」「で-、」といった表現などはやや特殊なようで、後でバイプロットにしたとき図が偏るため、ここでは集計から省いておきます。

```{r plot_balloon}
dfm <- df |>
  dplyr::group_by(doc_id) |>
  dplyr::mutate(POS1 = paste(POS1, dplyr::lead(POS1), sep = "-"),
                from = token, to = dplyr::lead(token)) |>
  dplyr::ungroup() |>
  dplyr::filter(POS1 %in% c("助詞-記号", "助動詞-記号", "接続詞-記号"),
                !from %in% c("と", "も", "で"),
                to %in% c("、", "。")) |>
  dplyr::count(author, from, to) |>
  dplyr::mutate(author = forcats::fct_drop(author),
                from = forcats::fct_lump(from, n = 12, w = n),
                term = paste(from, to, sep = "-")) |>
  tidytext::cast_dfm(author, term, n)

dfm |>
  quanteda::convert(to = "data.frame") |>
  tibble::column_to_rownames("doc_id") |>
  ggpubr::ggballoonplot()
```

## 対応分析

対応分析してバイプロットを描いてみます。

```{r plot_ca}
mdl <- dfm |>
  quanteda.textmodels::textmodel_ca()

require(ca)
plot(mdl)
```

## TF-IDF

「と-。」と「も-。」「で-、」のTF-IDFなどを確認します。ここでのTF-IDFなどは、すべてのトークンについてのものではなく、あくまでマイニングしているパターンのなかでの値です。

「と-。」と「も-。」に関しては、そもそもほとんど使われない表現のようです。

```{r tfidf1}
df |>
  dplyr::group_by(doc_id) |>
  dplyr::mutate(POS1 = paste(POS1, dplyr::lead(POS1), sep = "-"),
                from = token, to = dplyr::lead(token)) |>
  dplyr::ungroup() |>
  dplyr::filter(POS1 %in% c("助詞-記号", "助動詞-記号", "接続詞-記号")) |>
  dplyr::count(author, from, to) |>
  dplyr::mutate(term = paste(from, to, sep = "-")) |>
  tidytext::bind_tf_idf(term, author, n) |>
  dplyr::filter(term == "と-。") |>
  dplyr::select(author, term, n, tf, idf, tf_idf)
```

```{r tfidf2}
df |>
  dplyr::group_by(doc_id) |>
  dplyr::mutate(POS1 = paste(POS1, dplyr::lead(POS1), sep = "-"),
                from = token, to = dplyr::lead(token)) |>
  dplyr::ungroup() |>
  dplyr::filter(POS1 %in% c("助詞-記号", "助動詞-記号", "接続詞-記号")) |>
  dplyr::count(author, from, to) |>
  dplyr::mutate(term = paste(from, to, sep = "-")) |>
  tidytext::bind_tf_idf(term, author, n) |>
  dplyr::filter(term == "も-。") |>
  dplyr::select(author, term, n, tf, idf, tf_idf)
```

一方で、「で-、」という表現はどの作家も使ってはいるものの、TFを見るとすこし差があるように見えます。

```{r tfidf3}
df |>
  dplyr::group_by(doc_id) |>
  dplyr::mutate(POS1 = paste(POS1, dplyr::lead(POS1), sep = "-"),
                from = token, to = dplyr::lead(token)) |>
  dplyr::ungroup() |>
  dplyr::filter(POS1 %in% c("助詞-記号", "助動詞-記号", "接続詞-記号")) |>
  dplyr::count(author, from, to) |>
  dplyr::mutate(term = paste(from, to, sep = "-")) |>
  tidytext::bind_tf_idf(term, author, n) |>
  dplyr::filter(term == "で-、") |>
  dplyr::select(author, term, n, tf, idf, tf_idf)
```

## ロジスティック回帰

芥川龍之介（芥川竜之介）と太宰治の10,000字未満の作品について、ロジスティック回帰をしてみます。

```{r reload_file}
df <- readr::read_csv(tmp, col_types = "cccf") |>
  dplyr::mutate(author = as.factor(author)) |>
  dplyr::filter(nchar(text) < 10000,
                author %in% c("太宰治", "芥川竜之介")) |>
  dplyr::mutate(author = forcats::fct_drop(author))

summary(df)
```

先ほどと同様に形態素解析してから、「助詞-記号」「助動詞-記号」というパターンのみを残した分かち書きにします。

```{r tokenize_sp}
dfm <- df |>
  dplyr::mutate(text = audubon::strj_normalize(text)) |>
  dplyr::select(doc_id, text, author) |>
  gibasa::tokenize(split = TRUE) |>
  gibasa::prettify(col_select = "POS1") |>
  dplyr::group_by(doc_id) |>
  dplyr::mutate(POS1 = paste(POS1, dplyr::lead(POS1), sep = "-"),
                from = token, to = dplyr::lead(token)) |>
  dplyr::ungroup() |>
  dplyr::filter(POS1 %in% c("助詞-記号", "助動詞-記号")) |>
  dplyr::mutate(term = paste(from, to, sep = "-")) |>
  gibasa::pack(term)
```

分かち書きしたテキストをquantedaのコーパスにしつつ、適当なパターンの特徴量だけ残した文書単語行列にします。

```{r make_dfm}
dfm <- dfm |>
  dplyr::left_join(dplyr::select(df, doc_id, author), by = "doc_id") |>
  quanteda::corpus() |>
  quanteda::tokens(what = "fastestword", remove_punct = FALSE) |>
  quanteda::tokens_select(c("は-、", "が-、", "に-、", "て-、", "か-、", "で-、", "と-、", "ない-。", "た-。", "だ-。", "が-。")) |>
  quanteda::dfm() |>
  quanteda::dfm_weight(scheme = "prop")
```

`quanteda.textmodels::textmodel_lr`を使って学習します（glmnetのラッパーのようです）。

```{r train_mdl}
train <- quanteda::dfm_sample(dfm, size = 40, by = author)
test <- quanteda::dfm_subset(dfm, !quanteda::docid(dfm) %in% quanteda::docid(train))

mdl <- quanteda.textmodels::textmodel_lr(train, quanteda::docvars(train, "author"))
```

結果を確認すると、思ったよりそれっぽく予測できているように見えます。実際、「読点の前の文字の相対頻度」などは書き手の同定に使えるっぽいという報告はあるようなので、特徴量として残すパターンをもっと工夫すれば、けっこう精度良く予測できるのかもしれません。

```{r make_conf_mat}
res <-
  predict(mdl, test, type = "probability") |>
  as.data.frame() |>
  dplyr::filter(`太宰治` > .5) |>
  row.names()

df |>
  dplyr::filter(doc_id %in% quanteda::docid(test)) |> 
  dplyr::mutate(pred = dplyr::if_else(doc_id %in% res, "太宰治", "芥川竜之介")) |>
  dplyr::select(pred, author) |>
  table()
```

## セッション情報

```{r session_info}
sessioninfo::session_info()
```
