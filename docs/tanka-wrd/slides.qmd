---
title: "Rを使って短歌の「詩的度」を測る"
author: "あきる（paithiov909）"
date: "2024-02-24"
date-format: long
lang: ja
format:
  revealjs:
    transition: fade
    slide-number: "c/t"
    overview: true
    chalkboard: false
    progress: true
    history: false
    theme: [default, slides.scss]
    include-in-header: header.html
    embed-resources: true
---

## 誰？

::::{.columns}
:::{.column width="30%" class="py-12"}
![](https://rawcdn.githack.com/paithiov909/paithiov909/f5342cd61b45e29c34b17fc11c9bc1766eacb441/avatar.jpg){class="w-64 h-64 rounded-full"}
:::
:::{.column width="70%"}
- [paithiov909](https://github.com/paithiov909)というアカウントでインターネットしている
- Rパッケージをつくったりしている
  - [gibasa: A plain 'Rcpp' wrapper of 'MeCab'](https://github.com/paithiov909/gibasa/)
  - [audubon: An R package for Japanese text processing](https://github.com/paithiov909/audubon)
- 短歌をつくったり読んだりするのにはまっていた（過去形）
:::
::::

:::{.notes}
- このアイコンとアカウント名でGitHubやXにいます。今日は「あきる」という名前で参加している
- 趣味でRのパッケージを生やしている。この2つはCRANにもあるので、よかったら試してください
- 仕事でRを書いたりはしない。最近、なぜか職場でReactを書いたりしているが、本来はプログラムを書く職業ではない。Rを書くのは趣味みたいなもの
- 趣味「Rを書くこと」みたいな感じだが、もっと趣味らしい趣味としては、いっとき短歌をやっていた。過去形なので、最近はまったくやっていないが、今日は短歌とRを絡めた発表をする
:::

## どっちが「詩的」に見える？

::::{.columns}
:::{.column}
> 水星をのぞむ明け方　コンビニのＦＡＸに「故障中」の張り紙
>
> （五島諭『緑の祠』）

> ２月５日夜のコンビニ　暴力を含めてバランスを取る世界
>
> （永井祐『日本の中でたのしく暮らす』）

:::
:::{.column}
- 短歌は、しばしば前後2つの部分から構成されている（「句切れ」がある）
- そうした作品では、上句と下句とのあいだに意味的な飛躍がある場合がある
- ***「詩的」に感じられる「飛躍がある」をハックできれば、よい短歌がつくれるかも？***

:::
::::

:::{.notes}
- ここに2つの短歌を引用しました。みなさんは、どちらの短歌が好みですか？
- もちろん、どちらが好みかに正解はない。短歌は文芸作品なので、どちらが好みかは、その人の感性によって違ってあたりまえ。むしろ、人によって好みが違うからこそ、感想を語り合う楽しみがある
- それでも、どちらがよりよい短歌だろうかと考えたくなるのも、人のこころというもの。ここでは、2つの短歌を見せられたとして、どちらがよりよい短歌かというのをどのようにして判断するかを考えてみたい
- この2つの短歌をじっくり見てみると、どちらもコンビニという言葉を詠み込んでいて、空白がある。短歌には、このように句切れがあるつくりのものがある（もちろん、明確な句切れがないものもある）
- そうした作品では、前半と後半とのあいだに意味的な飛躍がある場合があり、そうしたレトリックを評して「詩的飛躍がある」と言ったりする
- この「詩的飛躍がある」をハックできれば、よい短歌がつくれるようになるかもしれない？
:::

## アイデア

::::{.columns}
:::{.column}
> ラスボスの手前でセーブするように無意味に入るファミリーマート

> 水槽にさかなを容れて飼うようにたぶんこのまま付かない既読

:::
:::{.column}
- 「詩的」のひとつの捉え方<br />***＝巧みな比喩的表現^[比喩的表現：同一言語内でよく見られる共起をあえて逸脱する表現によって、そのものごとについての新しい側面を言い表そうとする表現]が用いられている***
- 逸脱的な共起がなされている＝前後の意味的な隔たりが大きい場合、その表現は「詩的」だと期待できる
:::
::::

:::{.notes}
- 詩的であるとはどういうことかには、いろいろな立場がありそうだが、ひとつの捉え方として「巧みな比喩的表現が用いられている」というのはありそう
- 比喩的表現というのは、これもいろいろな定義ができそうだが、ここでは「同一言語内でよく見られる共起をあえて逸脱する表現によって、そのものごとについての新しい側面を言い表そうとする表現」としておく
- どういうことかというと、たとえば、このスライドに載せている短歌でいえば、「ファミリーマートに何を買うためでもなく入ること」について「ラスボスの手前でセーブするように」という喩えはあまりしなさそう。つまり、共起として逸脱的なので、それによって詩的に見える
- あるいは、下の例でいえば、「たぶんこのまま既読が付かないんだろうなと思いながらメッセージを眺めているようす」について「水槽にさかなを容れて飼うように」という喩えはあまりしなさそう。やはり、共起として稀に思われるので、詩的に見える表現になっている
- このように、逸脱的な共起がなされている＝前後の意味的な隔たりが大きい場合に、その表現は「詩的」なものと判断できるということがありそう
:::

## モチベーション

- 前後2つの文（単語列）のあいだの意味的な隔たりを定量的に評価できれば、その短歌が「詩的」かを判断する目安にできるかもしれない
  - 単語列間の非類似度（距離）を計算すればよさそう
  - そうした非類似度を計算するには、単語列をその意味が反映されたベクトル表現にすればよい

:::{.notes}
- このアイデアによるならば、短歌を前後2つの部分に分けたとき、それら2つの部分の意味的な隔たりを定量的に評価できれば、その短歌が「詩的」かを判断する目安にできるかもしれません
- これを試してみようというのが、この発表でやりたいこと
- これを実現するには、短歌を前後2つの部分に分けて得られる単語列間の非類似度を計算すればよさそう
- また、そうした非類似度を計算するには、単語列をその意味が反映されたベクトル表現にすればよい
- 単語列をベクトル表現にするというと難しそうだが、最近だと「埋め込み」という技術を使うことで簡単に実現できる
:::

## 自然言語処理における埋め込み

単語や文などを、それらの意味を表現するベクトル空間にマッピングする手法、そうした手法で得られるベクトル表現のことを ***埋め込み（embeddings）*** という

- 似た意味の語彙はベクトル空間のなかでも「近い」位置に写像される、「意味の演算」が可能であるといった特徴がある
- 今回は[chiVe](https://github.com/WorksApplications/chiVe)という単語埋め込みを使いつつ、単語列間の ***WRD*** を計算することによって「単語列間の非類似度」を得る

:::{.notes}
- 自然言語処理における「埋め込み」とは、単語や文などを、それらの意味を表現するベクトル空間にマッピングする手法、また、そうした手法で得られるベクトル表現のこと
- こうした埋め込みでは、直観的に意味の似ているラベルはベクトル空間のなかでも近い位置に写像される、意味の演算が可能である（ベクトルを足したり引いたりしても、その意味関係が保存される）という便利な特徴がある
- ここでは、chiVeという単語埋め込みを使いつつ、単語列間のWRDという尺度を計算することによって「単語列間の非類似度」を得る
:::

## chiVeの埋め込みの例

chiVeの単語ベクトルはこんな感じ。300次元のベクトルが得られる

```{r chive}
#| echo: true
#| code-fold: true
require(apportita)
chive_path <- path.expand("~/Downloads/models/magnitude/chive-1.2-mc90.magnitude")
conn <- magnitude(chive_path)
query(conn, c("新しい", "朝", "が", "来た"))
```

## Word Rotator's Distance（1）

- 2つの文A, Bのあいだの最適輸送を考えて、文の非類似度を計算する手法
  - 文の重み（確率変量）として、単語ベクトルのL2ノルムを正規化したものを使う
  - このとき、輸送コストとして、単語ベクトル間のコサイン距離^[「1」からコサイン類似度を引いて変換した値のことを「コサイン距離」という]を使う
- 詳しくは [[2004.15003] Word Rotator's Distance](https://arxiv.org/abs/2004.15003) を読んでください

:::{.notes}
- ここから道具立ての説明をしていく。WRDという尺度を使うと言ったので、WRDについて説明する。説明するが、発表者の中途半端な説明を聞いてもよくわからないと思われるので、ちゃんと理解したい人は、こちらの論文を読むのがオススメ
- WRDというのはWord Rotator's Distanceの略。2つの文A, Bのあいだの最適輸送を考えて、文の非類似度を計算する手法
- 最適輸送なので、輸送する重みと輸送にかかるコストというのがあって、文の重みとしては単語ベクトルのL2ノルムを正規化したものを使う
- また、コストとしては、単語ベクトル間のコサイン距離を使う
:::

## Word Rotator's Distance（2）

文Aの重みについて、対応する文Bの重みへと移し替えるような対応づけ（超球面上での回転）を考えて、得られたコストの総和の最小値を2つの文の非類似度とする

![図は[[2004.15003] Word Rotator's Distance](https://arxiv.org/abs/2004.15003)から抜粋](ot-image.jpg)

:::{.notes}
- イメージとしては、文Aの確率変量に適当な割合をかけながら、文Bの確率変量と同じになるように移し替えるみたいなことを考える。WRDの場合、これは文Aの重みを文Bの重みに適当に移し替えることで、単語を超球面上で回転させることに相当する
- このときに、重みを1単位移し替えるのにかかるコストは、移し替え先の単語によって変わるものとする。WRDでは2つの点（単語）がベクトル空間のなかでなす角が小さいほど小さく・大きいほど大きくなるようにコストを設定し、可能な移し替えのパターンの中から、移し替えをしたときにかかるコストの総和が最小になるものを見つける
:::

## Word Rotator's Distance（3） {auto-animate="true"}

輸送コスト（costm）は距離行列なのでWasserstein距離^[数式は[POTのドキュメント](https://pythonot.github.io/quickstart.html#computing-wasserstein-distance)から]としてWRDを計算できる（p=1）

```{r wasserstein}
#| echo: true
#| code-fold: true
s1 <- runif(6 * 300) |> matrix(6, 300) |> scale(center = FALSE)
s2 <- runif(4 * 300) |> matrix(4, 300) |> scale(center = FALSE)
# コサイン距離
d <- 1 - proxyC::simil(s1, s2, method = "cosine", use_nan = FALSE)
w1 <- (\() { x <- sqrt(rowSums(s1^2)); x / sum(x) })()
w2 <- (\() { x <- sqrt(rowSums(s2^2)); x / sum(x) })()
transport::wasserstein(w1, w2, p = 1, costm = d, prob = TRUE)
```

\begin{align}\begin{aligned}W_p(a,b)=(\min_{\gamma \in \mathbb{R}_+^{m\times n}} \sum_{i,j}\gamma_{i,j}\|x_i-y_j\|_p)^\frac{1}{p}\\s.t. \gamma 1 = a; \gamma^T 1= b; \gamma\geq 0\end{aligned}\end{align}

:::{.notes}
- 具体的には、WRDでは輸送コストはコサイン距離の距離行列だったので、WRDはWasserstein距離として計算することができる
- Wasserstein距離というのは、下の数式で示したようなやつ。ガンマというのがコストで、WRDの場合、pは1です
- Rでの実装例としては、このコードブロックのような感じで計算できる
- すこし詳しく見ると、たとえば、ふたつの文s1, s2の文ベクトルがそれぞれ6行300列（300次元で6単語）と4行300列（300次元4単語）で与えられていたとして、コサイン距離はこんな感じで計算できる
- 文の重みとしては、行ごと（単語ごと）のノルムをとって、その和でわって正規化する
- RでWasserstein距離を計算するには`transport::wasserstein`という関数にこんな感じで渡すと計算できる
- これは距離なので、まったく同じ文のあいだのWRDは0になって、2つの文の意味的な隔たりが大きいほどWRDの値も大きくなる
:::

## WRDを計算してみる（1）

先ほどの短歌について、全角スペースで区切る場合、五島の短歌のほうが前後の非類似度が大きい

```{r wrd_func}
#| echo: true
#| code-fold: true
sudachi <- sudachir::rebuild_tokenizer(mode = "C")
form <- \(x) {
  unlist(sudachir::form(x, type = "normalized", pos = FALSE, instance = sudachi))
}
wrd <- \(conn, s1, s2) {
  purrr::map2_dbl(s1, s2, \(chunk1, chunk2) {
    chunk1 <- form(chunk1)
    chunk2 <- form(chunk2)
    tryCatch(
      apportita::calc_wrd(conn, chunk1, chunk2),
      error = \(e) { NA }
    )
  }, .progress = FALSE)
}
```

```{r wrd1}
#| echo: true
#| code-fold: false
# 五島の短歌
wrd(conn, "水星をのぞむ明け方", "コンビニのＦＡＸに「故障中」の張り紙")
# 永井の短歌
wrd(conn, "２月５日夜のコンビニ", "暴力を含めてバランスを取る世界")
```

:::{.notes}
- これでWRDを計算できるので、実際に短歌について計算してみた結果をお見せします
- 冒頭で見せた短歌について、全角スペースで区切りながらWRDを計算するとこのようになる
- ここでは、上の五島の短歌のほうがWRDが大きい、つまり、前後の意味的な隔たりが大きいということ
- つまり、捉えようによっては、この2首だと上のほうが「詩的度」が高いといえる、かもしれない
- これで「意味的な隔たりを定量的に評価したい」という目的はとりあえず達成できたが、まだ考えるべきことがある
:::

## WRDを計算してみる（2）{auto-animate="true"}

一方で、WRDは単語列の長さの影響を受けるため、区切る位置によって値が変わってしまう。どのように区切るべきだろうか？

```{r wrd2}
#| echo: true
#| code-fold: false
wrd(conn, "ラスボスの手前でセーブするように", "無意味に入るファミリーマート")
wrd(conn, "ラスボスの手前でセーブするように無意味に入る", "ファミリーマート")
```

:::{.notes}
- というのも、WRDは単語列の長さの影響を受けるため、区切る位置によって値が変わってしまう
- たとえば、この短歌について、上のような無難な位置で区切って計算する場合、WRDはこのくらいの値になるが、下のように後半が1語ないし2語になる「偏った区切り」をして計算すると、WRDの値がずいぶん大きくなる
- 現実の短歌は、全角スペースで区切られている作品ばかりではなく、そうした短歌を人手で2つの文に区切ろうとすると、その人の気持ち次第でWRDの値が変化してしまう
- そもそも、人手で区切るとかやりたくない。どうしよう？
:::

## 短歌を文節で区切る（1）

短歌を人手で2つの文に区切るのではなく、可能な区切りすべてについて文のペアと見なし、それらのWRDを全部計算してみることにする

```{r split_func}
#| echo: true
#| code-fold: true
strj_split_boundaries <- \(x) {
  stringi::stri_split_boundaries(
    x, # ICU>=73.2でビルドしたstringiが必要
    opts_brkiter = stringi::stri_opts_brkiter(locale = "ja@lw=phrase;ld=auto")
  )
}
split_kugire <- \(x) {
  purrr::map(strj_split_boundaries(x), \(elem) {
    len <- length(elem)
    if (len < 2) {
      return(NA_character_)
    } else {
      sapply(seq_len(len - 1), \(i) {
        s1 <- paste0(elem[1:i], collapse = "")
        s2 <- paste0(elem[(i + 1):len], collapse = "")
        paste(s1, s2, sep = "\t")
      })
    }
  })
}
```

```{r split_example}
#| echo: false
split_kugire("水槽にさかなを容れて飼うようにたぶんこのまま付かない既読")
```

:::{.notes}
- そこでRを使って、あらかじめ短歌に区切り位置を与えることにする。そのうえで、可能な区切りすべてについてそれぞれ文のペアと見なし、それらのWRDを全部計算することにしたい
- ようするに、こんな感じで、日本語文字列に文節区切りを与えたい
- 一見難しそうだが、文節に近しい単位での分割は、最近だとstringiパッケージでできるようになった。stringrの裏で使われているパッケージ
- こんな感じの関数を書けばできる。注意点として、比較的新しいバージョンのICUでビルドされたstringiが必要なので、もし試してみて再現できなかったら、stringiをインストールしなおすとよい
:::

## 短歌を文節で区切る（2）

##### ふつうにWRDを集計した場合{class="pt-6 pl-3"}

```{r wrd3}
dat <- tibble::tibble(
  text = c(
    "水星をのぞむ明け方コンビニのFAXに「故障中」の張り紙",
    "2月5日夜のコンビニ暴力を含めてバランスを取る世界",
    "ラスボスの手前でセーブするように無意味に入るファミリーマート",
    "水槽にさかなを容れて飼うようにたぶんこのまま付かない既読"
  ),
  doc_id = factor(1:4, labels = text)
)
ret <- dat |>
  dplyr::reframe(
    doc_id = doc_id,
    phrase = unlist(split_kugire(text)),
    .by = doc_id
  ) |>
  tidyr::separate_wider_delim(
    phrase, delim = "\t", names = c("s1", "s2")
  ) |>
  dplyr::mutate(wrd = wrd(conn, s1, s2))

ret |>
  dplyr::summarise(
    `max` = max(wrd),
    `median` = median(wrd),
    `mean` = mean(wrd),
    `min` = min(wrd),
    `n` = dplyr::n(),
    .by = doc_id
  )
```

##### 前後ともに5文字以上あるペアだけにしぼった場合{class="pt-8 pl-3"}

```{r wrd4}
ret |>
  dplyr::filter(
    stringr::str_length(s1) > 4,
    stringr::str_length(s2) > 4
  ) |>
  dplyr::summarise(
    `max` = max(wrd),
    `median` = median(wrd),
    `mean` = mean(wrd),
    `min` = min(wrd),
    `n` = dplyr::n(),
    .by = doc_id
  )
```

:::{.notes}
- これで区切りをどうしよう問題は解決できたので、再度WRDを計算してみる
- すでに見せた4首の短歌について、いま説明したやり方で文のペアを得て、WRDを計算して集計した
- いい感じに計算できていそうだが、短歌ごとのWRDの最大値はやや高すぎる気もする
- これは、文節区切りがそもそもうまくいっていなかったり、未知語が含まれたりした結果おかしくなっているパターン
- ある程度はどうしようもないが、前後の文のどちらかが極端に短いペアを取り除いてから集計すると、よりそれっぽい代表値が得られるようだった
- そのため、以降の説明では「前後ともに5文字以上ある文ペア」について扱った結果を紹介する
:::

## 前後のWRDが大きい区切り方

字数の多い名詞句があったりして前後の語数に偏りがあると、WRDが大きくなってしまう

```{r tanka72}
#| echo: false
dat <-
  arrow::read_parquet("tanka72-wrd.parquet") |>
  dplyr::as_tibble() |>
  dplyr::filter(
    stringr::str_length(s1) > 4,
    stringr::str_length(s2) > 4
  )
```

```{r max3}
#| echo: false
dat |>
  dplyr::transmute(
    wrd = round(wrd, 3),
    text = paste(s1, s2, sep = "／"),
    author = author
  ) |>
  dplyr::slice_max(wrd, n = 3) |>
  knitr::kable()
```

:::{.notes}
- それっぽい尺度を得るやり方が固まったので、いろいろな短歌のWRDを計算した結果を見てみましょう
- これは、適当に収集した、25名の作者による現代短歌・計72首について、いままでに説明したやり方でWRDを計算し、WRDが大きくなった文ペア上位3つを抽出した結果
- 字数の多い名詞句があったりした結果、前後の語数に偏りがあるケースのように見えるが、感覚的には悪くないような気もする。2番目の短歌とかよい
:::

## 前後のWRDが小さい区切り方

同じような語句が繰り返し用いられている短歌では、前後のWRDが小さくなりやすい

```{r min3}
#| echo: false
dat |>
  dplyr::transmute(
    wrd = round(wrd, 3),
    text = paste(s1, s2, sep = "／"),
    author = author
  ) |>
  dplyr::slice_min(wrd, n = 3) |>
  knitr::kable()
```

:::{.notes}
- 一方で、同様の方法でWRDを計算して、値が小さくなった文ペアを順に3つ抽出した結果がこちら
- 区切りとしてはやや不自然に見えるが、前後の文を通じて同じような語句が繰り返し用いられている短歌では、値が小さくなっていることがわかる。これも直観的にはおかしくない
:::

## WRDの分布（1）

```{r dist1}
#| echo: false
#| fig-cap: "25名の作者による現代短歌、計72首について、同様の方法でWRDを計算した。作者名の後ろの数字はデータセットに含まれるその作者による作品の数"
require(ggplot2)

g <- dat |>
  dplyr::transmute(
    id = id,
    wrd = wrd,
    text = paste(s1, s2, sep = "／"),
    author = author
  ) |>
  dplyr::group_by(author) |>
  dplyr::mutate(
    author = paste(as.character(author), length(unique(id)), sep = "/")
  ) |>
  dplyr::ungroup() |>
  ggpubr::ggdensity(
    x = "wrd", color = "author",
    rug = TRUE, alpha = .5
  )

g +
  geomtextpath::geom_textvline(
    aes(xintercept = wrd, label = text, colour = author),
    data = \(x) dplyr::slice_max(x, wrd, n = 10),
    size = 3, linetype = "dashed", alpha = .6
  ) +
  coord_flip() +
  scale_color_viridis_d(option = "H") +
  theme_light()
```

:::{.notes}
- 次は、同様のデータセットについて、WRDの分布を描いてみたグラフです
- 何を伝えたい図なのか自分でもよくわからないので詳しくは見ませんが、どうやら、WRDのピークはこのあたりにあるように見えます
:::

## WRDの分布（2）

```{r dist2}
#| echo: false
#| fig-cap: "短歌投稿サイトの短歌74,844首と、発表者のツイート100件について、同様の方法でWRDを計算した"
require(patchwork)

tanka <-
  arrow::read_parquet("tanka-wrd.parquet") |>
  dplyr::as_tibble() |>
  dplyr::filter(
    stringr::str_length(s1) > 4, stringr::str_length(s2) > 4
  )
tweets <-
  arrow::read_parquet("shinabitanori-wrd.parquet") |>
  dplyr::as_tibble() |>
  dplyr::filter(
    stringr::str_length(s1) > 4, stringr::str_length(s2) > 4
  )

n_tanka <- nrow(tanka)
n_tweets <- nrow(tweets)

dat <- tanka |>
  dplyr::mutate(source = "tanka") |>
  dplyr::bind_rows(
    tweets |>
      dplyr::select(s1, s2, wrd) |>
      dplyr::mutate(source = "tweet")
  ) |>
  dplyr::mutate(
    source = factor(
      source,
      labels = c(
        paste0("短歌\n(n=", n_tanka, ")"),
        paste0("ツイート\n(n=", n_tweets, ")")
      )
    )
  )

g1 <-
  ggpubr::ggdensity(dat, x = "wrd", color = "source") +
  labs(title = "短歌とツイートのWRD") +
  scale_color_viridis_d(option = "H") +
  theme_light() +
  theme(legend.position = "bottom")

g2 <- tanka |>
  dplyr::summarise(
    wrd = max(wrd),
    .by = id
  ) |>
  ggpubr::ggdensity(x = "wrd") +
  labs(title = "短歌のWRD（最大値）") +
  scale_color_viridis_d(option = "H") +
  theme_light()

g1 + g2
```

:::{.notes}
- 詳しく確認するために、短歌のサンプルを増やして同様に分布を描いてみた
- 左のグラフは、短歌投稿サイトから収集した短歌74,844首について、同様の方法で計算したWRDの分布。比較用に、おおむね短歌と同じくらいの文字数と思われる発表者のツイート100件から計算したWRDの分布も描いています
- これを見ると、分布のピークは0.5よりちょっとだけ右にあって、右側の裾がちょっと膨らんでいるのがわかる
- これはどうやら短歌だからというのではなく、ツイートでも同じ
- 右のグラフは短歌ごとのWRDの最大値の分布だが、これを見るとやはり右裾の同じような位置が膨らんでいる
- 短歌くらいの文字数の日本語文字列を今回の方法で処理するかぎりでは、これくらいの割合で前後の意味的な隔たりの大きい文ペアが生じるということっぽい
:::

## まとめ

:::{.incremental}
- 短歌の「詩的度」の一側面を測れそうな尺度として、短歌を前後2つに区切った「文」のあいだのWRDを計算した
- 評価用データセットのようなものはないので客観的に評価できないが、感覚的には悪くない尺度に思われた
- WRDは計算に時間がかかるので、短歌がたくさんあると大変
- この方法で計算したWRDは、値が大きく・小さくなりやすいつくりに癖がありそうなので、短歌の「詩的度」を測るには別の観点も必要かも
:::

:::{.notes}
- そもそもの話、詩的度の高い「詩的な」短歌が必ずしも「よい短歌」だとはかぎらない
- 実際、短歌の世界はもっと複雑で、短歌をつくる人たちの多くは「より詩的な短歌をつくるバトル」をしているわけではない
- 今回は話の都合上、測りたい尺度として詩的度みたいなものを考えたが、測りたい尺度は本来、自分たちにとっての「よい短歌」にあたるものがどんなものかを考えたうえで決めるべき（それがわかっていればそんなに思い悩んだりしないという話ではあるが）
- その点、少なくとも私個人は、この値をハックして詩的に見えそうな短歌をつくるということについて、ゲームとしては面白そうだとは思うが、そんなに魅力を感じない
- 一方で、まず測れるか考えてみないとそれを測ってほんとにうれしいのか自分でもよく見えてこないというのもあるので、とりあえずどうやったら測れそうかを考えてみるのはチャレンジングで面白い課題だと思う
- みなさんもオレオレ詩的度の測り方を思いついたら教えてください
:::

:::{.center}
## Enjoy✨{class="text-center"}
:::

```{r close_conn}
#| echo: false
close(conn)
```
